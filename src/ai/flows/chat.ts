'use server';
/**
 * @fileOverview This flow handles the main chat functionality, including tool selection and calling the appropriate AI provider.
 * 
 * - chat - The main function that processes user queries.
 * - ChatInput - The input type for the chat function.
 * - ChatOutput - The return type for the chat function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';

const ChatInputSchema = z.object({
  query: z.string().describe('The user query to be processed by the AI tool.'),
  userId: z.string().describe('The ID of the user making the query.'),
  selectedTool: z.string().describe('The tool selected by the user. Can be "Auto".'),
  apiKey: z.string().optional().describe('The API key for the selected tool, if required.'),
});
export type ChatInput = z.infer<typeof ChatInputSchema>;

const ChatOutputSchema = z.object({
  tool: z.string().describe('The AI tool that was used.'),
  response: z.string().describe('The response generated by the selected AI tool.'),
  rawResponse: z.string().optional().describe('The raw response from the AI provider, for debugging.'),
});
export type ChatOutput = z.infer<typeof ChatOutputSchema>;

export async function chat(input: ChatInput): Promise<ChatOutput> {
  return chatFlow(input);
}

const selectAITool = ai.defineTool(
  {
    name: 'selectAITool',
    description: 'Selects the most appropriate AI tool based on the complexity and context of the user query.',
    inputSchema: z.object({
      query: z.string().describe('The user query to be processed.'),
    }),
    outputSchema: z.enum(['GPT', 'Gemini', 'Purplexcity', 'Grok', 'Deepseek', 'Hugging Face', 'Ollama']),
  },
  async (input) => {
    const llmResponse = await ai.generate({
      prompt: `Based on the following user query, which AI tool would be the most appropriate to use?

        User Query: "${input.query}"

        Available Tools:
        - GPT: Best for complex reasoning, and creative text generation.
        - Gemini: A powerful, general-purpose model good for a wide range of tasks.
        - Purplexcity: Specialized in search and information retrieval.
        - Grok: Good for conversational AI and humor.
        - Deepseek: Strong in coding and technical queries.
        - Hugging Face: Good for a wide variety of open models.
        - Ollama: For running local models.

        Select one tool from the list above.`,
      model: 'googleai/gemini-1.5-flash',
    });
    const selectedTool = llmResponse.text.trim();
    const validTools = ['GPT', 'Gemini', 'Purplexcity', 'Grok', 'Deepseek', 'Hugging Face', 'Ollama'];
    if (validTools.includes(selectedTool)) {
      return selectedTool as any;
    }
    return 'Gemini'; // Default fallback
  }
);

const chatFlow = ai.defineFlow(
  {
    name: 'chatFlow',
    inputSchema: ChatInputSchema,
    outputSchema: ChatOutputSchema,
  },
  async (input) => {
    let finalTool = input.selectedTool;

    if (finalTool === 'Auto') {
      finalTool = await selectAITool({ query: input.query });
    }

    let response: string = '';
    let rawResponse: any = {};
    let endpoint = '';
    let headers: Record<string, string> = { 'Content-Type': 'application/json' };
    let body: any = {};

    switch (finalTool) {
      case 'GPT':
        endpoint = 'https://api.openai.com/v1/chat/completions';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'gpt-4o-mini',
          messages: [{ role: 'user', content: input.query }],
        };
        break;
      case 'Gemini':
        endpoint = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${input.apiKey}`;
        body = {
          contents: [{ parts: [{ text: input.query }] }],
        };
        break;
      case 'Deepseek':
        endpoint = 'https://api.deepseek.com/chat/completions';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'deepseek-chat',
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: input.query },
          ],
          stream: false,
        };
        break;
      case 'Hugging Face':
        const model = 'mistralai/Mistral-7B-Instruct-v0.2'; // A default model
        endpoint = `https://api-inference.huggingface.co/models/${model}`;
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          inputs: input.query,
        };
        break;
      case 'Ollama':
        endpoint = 'http://localhost:11434/api/chat';
        if (input.apiKey) {
          headers['Authorization'] = `Bearer ${input.apiKey}`;
        }
        body = {
          model: 'llama2',
          messages: [
            { role: 'system', content: 'You are a helpful AI.' },
            { role: 'user', content: input.query },
          ],
        };
        break;
      case 'Grok':
        rawResponse = { note: 'This is a placeholder response as Grok API is not public.' };
        response = rawResponse.note;
        break;
      case 'Purplexcity':
        response = `Simulating response from Purplexcity for query: "${input.query}"`;
        rawResponse = { note: 'This is a placeholder response as Purplexcity API is not public.' };
        break;
      default:
        response = `The tool "${finalTool}" is not recognized. Please select a valid tool.`;
        rawResponse = { error: 'Invalid tool selected' };
        break;
    }

    if (endpoint) {
      try {
        const apiResponse = await fetch(endpoint, {
          method: 'POST',
          headers: headers,
          body: JSON.stringify(body),
        });

        if (!apiResponse.ok) {
          const errorBody = await apiResponse.text();
          console.error(`API call failed for ${finalTool}`, {
              status: apiResponse.status,
              statusText: apiResponse.statusText,
              body: errorBody
          });
          throw new Error(`API call failed with status: ${apiResponse.status} ${apiResponse.statusText}. Response: ${errorBody}`);
        }
        
        rawResponse = await apiResponse.json();

        switch (finalTool) {
          case 'GPT':
            response = rawResponse.choices[0]?.message?.content || 'No response from GPT';
            break;
          case 'Gemini':
            response = rawResponse.candidates[0]?.content?.parts[0]?.text || 'No response from Gemini';
            break;
          case 'Deepseek':
            response = rawResponse.choices[0]?.message?.content || 'No response from Deepseek';
            break;
          case 'Hugging Face':
            response = rawResponse[0]?.generated_text || 'No response from Hugging Face';
            break;
          case 'Ollama':
            response = rawResponse.message?.content || 'No response from Ollama';
            break;
        }
      } catch (error) {
        console.error(`Error calling ${finalTool} API:`, error);
        response = `Error communicating with ${finalTool}. Please check your API key and network connection. If using Ollama, ensure your local server is running.`;
        rawResponse = { error: error instanceof Error ? error.message : String(error) };
      }
    }


    return {
      tool: finalTool,
      response: response,
      rawResponse: JSON.stringify(rawResponse, null, 2),
    };
  }
);
