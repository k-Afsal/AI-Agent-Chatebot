

'use server';
/**
 * @fileOverview This flow handles the main chat functionality, including tool selection and calling the appropriate AI provider.
 * 
 * - chat - The main function that processes user queries.
 * - ChatInput - The input type for the chat function.
 * - ChatOutput - The return type for the chat function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';

const ChatInputSchema = z.object({
  query: z.string().describe('The user query to be processed by the AI tool.'),
  userId: z.string().describe('The ID of the user making the query.'),
  selectedTool: z.string().describe('The tool selected by the user. Can be "Auto".'),
  apiKey: z.string().optional().describe('The API key for the selected tool, if required.'),
  ollamaHost: z.string().optional().describe('The host for the Ollama API.'),
  ollamaModel: z.string().optional().describe('The model to use for Ollama.'),
});
export type ChatInput = z.infer<typeof ChatInputSchema>;

const ChatOutputSchema = z.object({
  tool: z.string().describe('The AI tool that was used.'),
  response: z.string().describe('The response generated by the selected AI tool.'),
  rawResponse: z.string().optional().describe('The raw response from the AI provider, for debugging.'),
});
export type ChatOutput = z.infer<typeof ChatOutputSchema>;

export async function chat(input: ChatInput): Promise<ChatOutput> {
  return chatFlow(input);
}

const selectAITool = ai.defineTool(
  {
    name: 'selectAITool',
    description: 'Selects the most appropriate AI tool based on the complexity and context of the user query.',
    inputSchema: z.object({
      query: z.string().describe('The user query to be processed.'),
    }),
    outputSchema: z.enum(['GPT', 'Gemini', 'Deepseek', 'Ollama', 'OpenRouter', 'Cohere']),
  },
  async (input) => {
    const llmResponse = await ai.generate({
      prompt: `Based on the following user query, which AI tool would be the most appropriate to use?

        User Query: "${input.query}"

        Available Tools:
        - GPT: Best for complex reasoning, and creative text generation.
        - Gemini: A powerful, general-purpose model good for a wide range of tasks.
        - Deepseek: Strong in coding and technical queries.
        - Ollama: For running local models.
        - OpenRouter: Access to a wide variety of models, good for experimentation.
        - Cohere: Powerful models for enterprise use cases.

        Select one tool from the list above.`,
      model: 'googleai/gemini-1.5-flash',
    });
    const selectedTool = llmResponse.text.trim();
    const validTools = ['GPT', 'Gemini', 'Deepseek', 'Ollama', 'OpenRouter', 'Cohere'];
    if (validTools.includes(selectedTool)) {
      return selectedTool as any;
    }
    return 'Gemini'; // Default fallback
  }
);

const chatFlow = ai.defineFlow(
  {
    name: 'chatFlow',
    inputSchema: ChatInputSchema,
    outputSchema: ChatOutputSchema,
  },
  async (input) => {
    let finalTool = input.selectedTool;

    if (finalTool === 'Auto') {
      finalTool = await selectAITool({ query: input.query });
    }

    let response: string = '';
    let rawResponse: any = {};
    let endpoint = '';
    let headers: Record<string, string> = { 'Content-Type': 'application/json' };
    let body: any = {};

    switch (finalTool) {
      case 'GPT':
        endpoint = 'https://api.openai.com/v1/chat/completions';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: input.query }],
        };
        break;
      case 'Gemini':
        endpoint = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent`;
        if (input.apiKey) {
            headers['x-goog-api-key'] = input.apiKey;
        }
        body = {
          contents: [{ parts: [{ text: input.query }] }],
        };
        break;
      case 'Deepseek':
        endpoint = 'https://api.deepseek.com/v1/chat/completions';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'deepseek-chat',
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: input.query },
          ],
          stream: false,
        };
        break;
      case 'Ollama':
        const ollamaBaseUrl = input.ollamaHost || 'http://localhost:11434';
        endpoint = `${ollamaBaseUrl.replace(/\/$/, '')}/api/chat`;
        if (input.apiKey) {
          headers['Authorization'] = `Bearer ${input.apiKey}`;
        }
        body = {
          model: input.ollamaModel || 'llama2',
          messages: [
            { role: 'system', content: 'You are a helpful AI.' },
            { role: 'user', content: input.query },
          ],
        };
        break;
      case 'OpenRouter':
        endpoint = 'https://openrouter.ai/api/v1/chat/completions';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'google/gemini-flash-1.5', // A good default, can be changed
          messages: [{ role: 'user', content: input.query }],
        };
        break;
      case 'Cohere':
        endpoint = 'https://api.cohere.com/v1/chat';
        headers['Authorization'] = `Bearer ${input.apiKey}`;
        body = {
          model: 'command-r-plus',
          messages: [
            { "role": "user", "content": input.query }
          ],
        };
        break;
      default:
        response = `The tool "${finalTool}" is not recognized. Please select a valid tool.`;
        rawResponse = { error: 'Invalid tool selected' };
        break;
    }

    if (endpoint) {
      try {
        const apiResponse = await fetch(endpoint, {
          method: 'POST',
          headers: headers,
          body: JSON.stringify(body),
        });

        if (!apiResponse.ok) {
          const errorBody = await apiResponse.text();
          console.error(`API call failed for ${finalTool}`, {
              status: apiResponse.status,
              statusText: apiResponse.statusText,
              body: errorBody
          });
          throw new Error(`API call failed with status: ${apiResponse.status} ${apiResponse.statusText}. Response: ${errorBody}`);
        }
        
        rawResponse = await apiResponse.json();

        switch (finalTool) {
          case 'GPT':
          case 'OpenRouter':
            response = rawResponse.choices[0]?.message?.content || `No response from ${finalTool}`;
            break;
          case 'Deepseek':
            response = rawResponse.choices[0]?.message?.content || `No response from ${finalTool}`;
            break;
          case 'Gemini':
            response = rawResponse.candidates[0]?.content?.parts[0]?.text || 'No response from Gemini';
            break;
          case 'Ollama':
            response = rawResponse.message?.content || 'No response from Ollama';
            break;
          case 'Cohere':
            response = rawResponse.choices[0]?.message?.content || rawResponse.text || `No response from ${finalTool}`;
            break;
        }
      } catch (error) {
        console.error(`Error calling ${finalTool} API:`, error);
        if (finalTool === 'Ollama' && error instanceof TypeError && error.message.includes('fetch failed')) {
            response = `Error communicating with Ollama. Please ensure the server is running and accessible. If running locally, try using your machine's local IP address (e.g. http://192.168.1.5:11434) instead of localhost.`;
        } else if (error instanceof TypeError && error.message.includes('fetch failed')) {
            response = `Error communicating with ${finalTool}. Please ensure the local server is running and accessible at ${endpoint}.`;
        } else {
            response = `Error communicating with ${finalTool}. Please check your API key and network connection.`;
        }
        rawResponse = { error: error instanceof Error ? error.message : String(error) };
      }
    }


    return {
      tool: finalTool,
      response: response,
      rawResponse: JSON.stringify(rawResponse, null, 2),
    };
  }
);
